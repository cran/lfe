
R Under development (unstable) (2016-04-25 r70547) -- "Unsuffered Consequences"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "lfe"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('lfe')
Loading required package: Matrix
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("bccorr")
> ### * bccorr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bccorr
> ### Title: Compute limited mobility bias corrected correlation between
> ###   fixed effects
> ### Aliases: bccorr
> 
> ### ** Examples
> 
> x <- rnorm(500)
> x2 <- rnorm(length(x))
> 
> ## create individual and firm
> id <- factor(sample(40,length(x),replace=TRUE))
> firm <- factor(sample(30,length(x),replace=TRUE,prob=c(2,rep(1,29))))
> foo <- factor(sample(20,length(x),replace=TRUE))
> ## effects
> id.eff <- rnorm(nlevels(id))
> firm.eff <- rnorm(nlevels(firm))
> foo.eff <- rnorm(nlevels(foo))
> ## left hand side
> y <- x + 0.25*x2 + id.eff[id] + firm.eff[firm] + foo.eff[foo] + rnorm(length(x))
> 
> # make a data frame
> fr <- data.frame(y,x,x2,id,firm,foo)
> ## estimate and print result
> est <- felm(y ~ x+x2|id+firm+foo, data=fr, keepX=TRUE)
> # find bias corrections
> bccorr(est)
        corr           v1           v2          cov           d1           d2 
-0.032262721  0.952892562  1.376031164 -0.036943420  0.088877778  0.082733214 
         d12 
-0.004461406 
> 
> 
> 
> cleanEx()
> nameEx("btrap")
> ### * btrap
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: btrap
> ### Title: Bootstrap standard errors for the group fixed effects
> ### Aliases: btrap
> 
> ### ** Examples
> 
> 
> oldopts <- options(lfe.threads=2)
> ## create covariates
> x <- rnorm(3000)
> x2 <- rnorm(length(x))
> 
> ## create individual and firm
> id <- factor(sample(700,length(x),replace=TRUE))
> firm <- factor(sample(300,length(x),replace=TRUE))
> 
> ## effects
> id.eff <- rlnorm(nlevels(id))
> firm.eff <- rexp(nlevels(firm))
> 
> ## left hand side
> y <- x + 0.25*x2 + id.eff[id] + firm.eff[firm] + rnorm(length(x))
> 
> ## estimate and print result
> est <- felm(y ~ x+x2 | id + firm)
> summary(est)

Call:
   felm(formula = y ~ x + x2 | id + firm) 

Residuals:
    Min      1Q  Median      3Q     Max 
-3.3048 -0.5541  0.0000  0.5560  2.6463 

Coefficients:
   Estimate Std. Error t value Pr(>|t|)    
x   1.01027    0.02219   45.52   <2e-16 ***
x2  0.23879    0.02319   10.30   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.032 on 2010 degrees of freedom
Multiple R-squared(full model): 0.8997   Adjusted R-squared: 0.8503 
Multiple R-squared(proj model): 0.5262   Adjusted R-squared: 0.293 
F-statistic(full model):18.23 on 989 and 2010 DF, p-value: < 2.2e-16 
F-statistic(proj model):  1116 on 2 and 2010 DF, p-value: < 2.2e-16 


> ## extract the group effects
> alpha <- getfe(est)
> head(alpha)
       effect obs comp fe idx
id.1 2.780164   4    1 id   1
id.2 1.305613   2    1 id   2
id.3 1.973887   3    1 id   3
id.4 3.479075   4    1 id   4
id.5 2.618930   2    1 id   5
id.6 4.341930   3    1 id   6
> ## bootstrap standard errors
> head(btrap(alpha,est))
       effect obs comp fe idx        se
id.1 2.780164   4    1 id   1 0.4524265
id.2 1.305613   2    1 id   2 0.7353362
id.3 1.973887   3    1 id   3 0.5261809
id.4 3.479075   4    1 id   4 0.4598114
id.5 2.618930   2    1 id   5 0.6978330
id.6 4.341930   3    1 id   6 0.5812627
> 
> ## bootstrap some differences
> ef <- function(v,addnames) {
+   w <- c(v[2]-v[1],v[3]-v[2],v[3]-v[1])
+   if(addnames) {
+      names(w) <-c('id2-id1','id3-id2','id3-id1')
+      attr(w,'extra') <- list(note=c('line1','line2','line3'))
+   }
+   w
+ }
> # check that it's estimable
> is.estimable(ef,est$fe)
[1] TRUE
> 
> head(btrap(alpha,est,ef=ef))
            effect  note        se
id2-id1 -1.4745509 line1 0.7238747
id3-id2  0.6682742 line2 0.8375219
id3-id1 -0.8062767 line3 0.5451103
> options(oldopts)
> 
> 
> 
> 
> cleanEx()
> nameEx("cgsolve")
> ### * cgsolve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cgsolve
> ### Title: Solve a symmetric linear system with the conjugate gradient
> ###   method
> ### Aliases: cgsolve
> 
> ### ** Examples
> 
> 
>   N <- 100000
> # create some factors
>   f1 <- factor(sample(34000,N,replace=TRUE))
>   f2 <- factor(sample(25000,N,replace=TRUE))
> # a matrix of dummies, which probably is rank deficient
>   B <- makeDmatrix(list(f1,f2))
>   dim(B)
[1] 100000  56723
> # create a right hand side
>   b <- as.matrix(B %*% rnorm(ncol(B)))
> # solve B' B x = B' b
>   sol <- cgsolve(crossprod(B), crossprod(B, b), eps=-1e-2)
>   #verify solution
>   sqrt(sum((B %*% sol - b)^2))
[1] 0.0004464837
> 
> 
> 
> 
> cleanEx()
> nameEx("chainsubset")
> ### * chainsubset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: chainsubset
> ### Title: Chain subset conditions
> ### Aliases: chainsubset
> 
> ### ** Examples
> 
> N <- 10000
> dat <- data.frame(y=rnorm(N), x=rnorm(N),id=factor(sample(N/100,N,replace=TRUE)))
> # It's not the same as and'ing the conditions:
> ss <- chainsubset(x < mean(y), y < 3*mean(x))
> sum(eval(ss,dat))
[1] 56
> sum(evalq(x < mean(y) & y < 3*mean(x), dat))
[1] 2525
> ss2 <- chainsubset(x < mean(y), y < a*mean(x), out.vars='a')
> a <- 3; sum(eval(ss2, dat))
[1] 56
> a <- 2; sum(eval(ss2, dat))
[1] 317
> # Among observations with x < y, find entire id's with more than
> # one fifth of their x's larger than 1/2
> ss3 <- chainsubset( x < y, tapply(x,id,function(.xx) {sum(.xx > 1/2) > length(.xx)/5} )[id])
>  sum(eval(ss3,dat))
[1] 52
> 
> 
> 
> cleanEx()
> nameEx("compfactor")
> ### * compfactor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: compfactor
> ### Title: Find the connected components
> ### Aliases: compfactor
> 
> ### ** Examples
> 
> 
> ## create two factors
> f1 <- factor(sample(300,400,replace=TRUE))
> f2 <- factor(sample(300,400,replace=TRUE))
> 
> ## find the components
> cf <- compfactor(list(f1=f1,f2=f2))
> 
> ## show the third largest component
> fr <- data.frame(f1,f2,cf)
> fr[cf==3,]
     f1  f2 cf
55   22  17  3
100 182  17  3
179 182 253  3
252 280 253  3
254 182 118  3
367  22 170  3
> 
> 
> 
> 
> cleanEx()
> nameEx("condfstat")
> ### * condfstat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: condfstat
> ### Title: Compute conditional F statistic for weak instruments in an
> ###   IV-estimation with multiple endogenous variables
> ### Aliases: condfstat
> 
> ### ** Examples
> 
> 
> z1 <- rnorm(4000)
> z2 <- rnorm(length(z1))
> u <- rnorm(length(z1))
> # make x1, x2 correlated with errors u
> 
> x1 <- z1 + z2 + 0.2*u + rnorm(length(z1))
> x2 <- z1 + 0.94*z2 - 0.3*u + rnorm(length(z1))
> y <- x1 + x2 + u
> est <- felm(y ~ 1 | 0 | (x1 | x2 ~ z1 + z2))
> summary(est)

Call:
   felm(formula = y ~ 1 | 0 | (x1 | x2 ~ z1 + z2)) 

Residuals:
    Min      1Q  Median      3Q     Max 
-4.3972 -0.7110 -0.0092  0.7191  3.5336 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.01607    0.01962  -0.819 0.412825    
`x1(fit)`    0.90461    0.24097   3.754 0.000177 ***
`x2(fit)`    1.11122    0.25643   4.333  1.5e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.047 on 3997 degrees of freedom
Multiple R-squared(full model): 0.8992   Adjusted R-squared: 0.8992 
Multiple R-squared(proj model): 0.8992   Adjusted R-squared: 0.8992 
F-statistic(full model):1.479e+04 on 2 and 3997 DF, p-value: < 2.2e-16 
F-statistic(proj model): 1.479e+04 on 2 and 3997 DF, p-value: < 2.2e-16 
F-statistic(endog. vars):1.479e+04 on 2 and 3997 DF, p-value: < 2.2e-16 


> ## Not run: 
> ##D summary(est$stage1, lhs='x1')
> ##D summary(est$stage1, lhs='x2')
> ## End(Not run)
> 
> # the joint significance of the instruments in both the first stages are ok:
> t(sapply(est$stage1$lhs, function(lh) waldtest(est$stage1, ~z1|z2, lhs=lh)))
   p     chi2 df1 p.F        F  df2
x1 0 8416.522   2   0 4208.261 3997
x2 0 6889.635   2   0 3444.817 3997
> # everything above looks fine, t-tests for instruments, 
> # as well as F-tests for excluded instruments in the 1st stages.
> # The conditional F-test reveals that the instruments are jointly weak
> # (it's close to being only one instrument, z1+z2, for both x1 and x2)
> condfstat(est, quantiles=c(0.05, 0.95))
            x1       x2
iid F 8.023097 8.021403
attr(,"df1")
[1] 1
attr(,"quantiles")
          5%      95%
x1 0.2652038 1.233032
x2 0.7175800 1.720472
attr(,"quantiles")attr(,"q")
[1] 0.05 0.95
attr(,"quantiles")attr(,"samples")
[1] 100
> 
> 
> 
> 
> cleanEx()
> nameEx("demeanlist")
> ### * demeanlist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: demeanlist
> ### Title: Centre vectors on multiple groups
> ### Aliases: demeanlist
> 
> ### ** Examples
> 
> oldopts <- options(lfe.threads=1)
> ## create a matrix
> mtx <- data.frame(matrix(rnorm(999),ncol=3))
> # a list of factors
> rgb <- c('red','green','blue')
> fl <- replicate(4, factor(sample(rgb,nrow(mtx),replace=TRUE)), simplify=FALSE)
> names(fl) <- paste('g',seq_along(fl),sep='')
> # centre on all means
> mtx0 <- demeanlist(mtx,fl)
> head(data.frame(mtx0,fl))
          X1          X2          X3    g1    g2    g3    g4
1 -0.8668450  1.50762202 -1.08898096   red   red green   red
2  0.0825542 -0.38986296  0.66802164 green  blue  blue   red
3 -0.7869389  0.79517615  0.05985494  blue   red   red green
4  1.8280256  0.97790753  1.82237342  blue green  blue  blue
5  0.4935794 -0.05185021 -1.20979000  blue  blue green green
6 -1.0602976 -0.28754378  1.13633703 green   red  blue   red
> # verify that the group means for the columns are zero
> lapply(fl, function(f) apply(mtx0,2,tapply,f,mean))
$g1
                 X1            X2            X3
blue   1.832421e-12 -5.045401e-11  1.211166e-11
green  1.862484e-12 -4.625585e-11  5.738506e-12
red   -3.115460e-12  8.128143e-11 -1.470903e-11

$g2
                 X1           X2            X3
blue  -2.051028e-17 9.444344e-18 -3.574544e-17
green -8.673617e-18 3.469447e-19  5.172753e-18
red   -6.370692e-18 3.396477e-17  1.075067e-17

$g3
                 X1            X2            X3
blue  -1.744820e-11  2.885894e-11 -1.650047e-11
green  1.293615e-11 -5.983123e-12  1.088000e-11
red    3.143348e-12 -2.130632e-11  4.387097e-12

$g4
                 X1            X2            X3
blue   3.539578e-12 -1.447864e-12  2.761446e-12
green  1.931682e-12 -3.174342e-12  2.132959e-12
red   -6.139949e-12  5.127238e-12 -5.476854e-12

> options(oldopts)
> 
> 
> 
> cleanEx()
> nameEx("efactory")
> ### * efactory
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: efactory
> ### Title: Create estimable function
> ### Aliases: efactory
> 
> ### ** Examples
> 
> 
> oldopts <- options(lfe.threads=1)
> id <- factor(sample(5000,50000,replace=TRUE))
> firm <- factor(sample(3000,50000,replace=TRUE))
> fl <- list(id=id,firm=firm)
> obj <- list(fe=fl,cfactor=compfactor(fl))
> ## the trivial least-norm  transformtion, which by the way is non-estimable
> print(ef <- efactory(obj,'ln'))
function (v, addnames) 
{
    if (addnames) {
        names(v) <- nm
        attr(v, "extra") <- list(obs = obs, comp = comp, fe = fef, 
            idx = idx)
    }
    v
}
<bytecode: 0x72b7228>
<environment: 0x7521358>
> is.estimable(ef,fl)
Warning in is.estimable(ef, fl) :
  non-estimable function, largest error 0.006 in coordinate 4013 ("id.4013")
[1] FALSE
> ## then the default
> print(ef <- efactory(obj,'ref'))
function (v, addnames) 
{
    esum <- sum(v[extrarefs])
    df <- v[refsubs]
    sub <- ifelse(is.na(df), 0, df)
    df <- v[refsuba]
    add <- ifelse(is.na(df), 0, df + esum)
    v <- v - sub + add
    if (addnames) {
        names(v) <- nm
        attr(v, "extra") <- list(obs = obs, comp = comp, fe = fef, 
            idx = idx)
    }
    v
}
<bytecode: 0x6f152a0>
<environment: 0x7529520>
attr(,"verified")
[1] TRUE
> is.estimable(ef,fl)
[1] TRUE
> # get the names of the coefficients, i.e. the nm-variable in the function
> head(evalq(nm,environment(ef)))
[1] "id.1" "id.2" "id.3" "id.4" "id.5" "id.6"
> options(oldopts)
> 
> 
> 
> 
> cleanEx()
> nameEx("felm")
> ### * felm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: felm
> ### Title: Fit a linear model with multiple group fixed effects
> ### Aliases: felm
> 
> ### ** Examples
> 
> 
> oldopts <- options(lfe.threads=1)
> ## create covariates
> x <- rnorm(1000)
> x2 <- rnorm(length(x))
> 
> ## individual and firm
> id <- factor(sample(20,length(x),replace=TRUE))
> firm <- factor(sample(13,length(x),replace=TRUE))
> 
> ## effects for them
> id.eff <- rnorm(nlevels(id))
> firm.eff <- rnorm(nlevels(firm))
> 
> ## left hand side
> u <- rnorm(length(x))
> y <- x + 0.5*x2 + id.eff[id] + firm.eff[firm] + u
> 
> ## estimate and print result
> est <- felm(y ~ x+x2| id + firm)
> summary(est)

Call:
   felm(formula = y ~ x + x2 | id + firm) 

Residuals:
    Min      1Q  Median      3Q     Max 
-3.3746 -0.6739  0.0036  0.6755  3.1556 

Coefficients:
   Estimate Std. Error t value Pr(>|t|)    
x   1.05003    0.03185   32.97   <2e-16 ***
x2  0.44664    0.03183   14.03   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.029 on 966 degrees of freedom
Multiple R-squared(full model): 0.8067   Adjusted R-squared: 0.8001 
Multiple R-squared(proj model): 0.5728   Adjusted R-squared: 0.5583 
F-statistic(full model):122.2 on 33 and 966 DF, p-value: < 2.2e-16 
F-statistic(proj model): 647.7 on 2 and 966 DF, p-value: < 2.2e-16 


> ## Not run: 
> ##D ## compare with lm
> ##D summary(lm(y ~ x + x2 + id + firm-1))
> ## End(Not run)
> 
> # make an example with 'reverse causation'
> # Q and W are instrumented by x3 and the factor x4. Report robust s.e.
> x3 <- rnorm(length(x))
> x4 <- sample(12,length(x),replace=TRUE)
> 
> Q <- 0.3*x3 + x + 0.2*x2 + id.eff[id] + 0.3*log(x4) - 0.3*y + rnorm(length(x),sd=0.3)
> W <- 0.7*x3 - 2*x + 0.1*x2 - 0.7*id.eff[id] + 0.8*cos(x4) - 0.2*y+ rnorm(length(x),sd=0.6)
> 
> # add them to the outcome
> y <- y + Q + W
> 
> ivest <- felm(y ~ x + x2 | id+firm | (Q|W ~x3+factor(x4)))
> summary(ivest,robust=TRUE)

Call:
   felm(formula = y ~ x + x2 | id + firm | (Q | W ~ x3 + factor(x4))) 

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4146 -0.6686 -0.0149  0.6829  3.0998 

Coefficients:
         Estimate Robust s.e t value Pr(>|t|)    
x         0.89520    0.18857   4.747 2.37e-06 ***
x2        0.44111    0.03226  13.675  < 2e-16 ***
`Q(fit)`  1.02579    0.12955   7.918 6.61e-15 ***
`W(fit)`  0.93813    0.05105  18.375  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.025 on 964 degrees of freedom
Multiple R-squared(full model): 0.7459   Adjusted R-squared: 0.7367 
Multiple R-squared(proj model): 0.5609   Adjusted R-squared: 0.5449 
F-statistic(full model, *iid*):90.04 on 35 and 964 DF, p-value: < 2.2e-16 
F-statistic(proj model): 381.2 on 4 and 964 DF, p-value: < 2.2e-16 
F-statistic(endog. vars):547.8 on 2 and 964 DF, p-value: < 2.2e-16 


> condfstat(ivest)
             Q        W
iid F 36.65646 58.45394
attr(,"df1")
[1] 11
> ## Not run: 
> ##D # compare with the not instrumented fit:
> ##D summary(felm(y ~ x + x2 +Q + W |id+firm))
> ## End(Not run)
> options(oldopts)
> 
> 
> 
> 
> cleanEx()
> nameEx("fevcov")
> ### * fevcov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fevcov
> ### Title: Compute limited mobility bias corrected covariance matrix
> ###   between fixed effects
> ### Aliases: fevcov
> 
> ### ** Examples
> 
> 
> x <- rnorm(5000)
> x2 <- rnorm(length(x))
> 
> ## create individual and firm
> id <- factor(sample(40,length(x),replace=TRUE))
> firm <- factor(sample(30,length(x),replace=TRUE,prob=c(2,rep(1,29))))
> foo <- factor(sample(20,length(x),replace=TRUE))
> ## effects
> id.eff <- rnorm(nlevels(id))
> firm.eff <- runif(nlevels(firm))
> foo.eff <- rchisq(nlevels(foo),df=1)
> ## left hand side
> id.m <- id.eff[id]
> firm.m <- firm.eff[firm]
> foo.m <- foo.eff[foo]
> # normalize them
> id.m <- id.m/sd(id.m)
> firm.m <- firm.m/sd(firm.m)
> foo.m <- foo.m/sd(foo.m)
> y <- x + 0.25*x2 + id.m + firm.m + foo.m + rnorm(length(x),sd=2)
> z <- x + 0.5*x2 + 0.7*id.m + 0.5*firm.m + 0.3*foo.m + rnorm(length(x),sd=2)
> # make a data frame
> fr <- data.frame(y,z,x,x2,id,firm,foo)
> ## estimate and print result
> est <- felm(y|z ~ x+x2|id+firm+foo, data=fr, keepX=TRUE)
> # find bias corrections, there's little bias in this example
> print(yv <- fevcov(est, lhs='y'))
              id         firm          foo
id    0.92015755 -0.011173022 -0.019789538
firm -0.01117302  0.963808718 -0.009781304
foo  -0.01978954 -0.009781304  1.035240984
attr(,"bias")
                id          firm           foo
id    0.0319481064 -2.052376e-04 -1.023073e-04
firm -0.0002052376  2.187053e-02 -4.728025e-05
foo  -0.0001023073 -4.728025e-05  1.560268e-02
> ## Here's how to compute the unbiased correlation matrix:
> cm <- cov2cor(yv)
> structure(cm,bias=NULL)
              id         firm          foo
id    1.00000000 -0.011864355 -0.020276072
firm -0.01186435  1.000000000 -0.009792208
foo  -0.02027607 -0.009792208  1.000000000
> 
> 
> 
> 
> cleanEx()
> nameEx("getfe")
> ### * getfe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getfe
> ### Title: Retrieve the group fixed effects
> ### Aliases: getfe
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> oldopts <- options(lfe.threads=2)
> ## create covariates
> x <- rnorm(4000)
> x2 <- rnorm(length(x))
> 
> ## create individual and firm
> id <- factor(sample(500,length(x),replace=TRUE))
> firm <- factor(sample(300,length(x),replace=TRUE))
> 
> ## effects
> id.eff <- rlnorm(nlevels(id))
> firm.eff <- rexp(nlevels(firm))
> 
> ## left hand side
> y <- x + 0.25*x2 + id.eff[id] + firm.eff[firm] + rnorm(length(x))
> 
> ## estimate and print result
> est <- felm(y ~ x+x2 | id + firm)
> summary(est)

Call:
   felm(formula = y ~ x + x2 | id + firm) 

Residuals:
    Min      1Q  Median      3Q     Max 
-2.9142 -0.5841 -0.0093  0.5791  3.1733 

Coefficients:
   Estimate Std. Error t value Pr(>|t|)    
x   1.00768    0.01684   59.84   <2e-16 ***
x2  0.25075    0.01716   14.61   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9838 on 3199 degrees of freedom
Multiple R-squared(full model): 0.9275   Adjusted R-squared: 0.9094 
Multiple R-squared(proj model): 0.5433   Adjusted R-squared: 0.429 
F-statistic(full model):51.19 on 800 and 3199 DF, p-value: < 2.2e-16 
F-statistic(proj model):  1902 on 2 and 3199 DF, p-value: < 2.2e-16 


> ## extract the group effects
> alpha <- getfe(est,se=TRUE)
> 
> ## find some estimable functions, with standard errors, we don't get
> ## names so we must precompute some numerical indices in ef
> idx <- match(c('id.5','id.6','firm.11','firm.12'),rownames(alpha))
> alpha[idx,]
             effect obs comp   fe idx        se
id.5     1.13152965   9    1   id   5 0.3445724
id.6     2.14526185   7    1   id   6 0.3818530
firm.11 -0.07364465  19    1 firm  11 0.2777234
firm.12  0.85743509  13    1 firm  12 0.3326070
> ef <- function(v,addnames) {
+   w <- c(v[idx[[2]]]-v[idx[[1]]],v[idx[[4]]]+v[idx[[1]]],
+          v[idx[[4]]]-v[idx[[3]]])
+   if(addnames) names(w) <-c('id6-id5','f12+id5','f12-f11')
+   w
+ }
> getfe(est,ef=ef,se=TRUE)
           effect        se
id6-id5 1.0137322 0.4800143
f12+id5 1.9889647 0.3895297
f12-f11 0.9310797 0.3126626
> options(oldopts)
> ## Not run: 
> ##D summary(lm(y ~ x+x2+id+firm-1))
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("is.estimable")
> ### * is.estimable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: is.estimable
> ### Title: Verify estimability of function
> ### Aliases: is.estimable
> 
> ### ** Examples
> 
> 
> oldopts <- options(lfe.threads=1)
> ## create individual and firm
> id <- factor(sample(5000,50000,replace=TRUE))
> firm <- factor(sample(3000,50000,replace=TRUE))
> 
> ## create some estimable functions. It's faster to
> ## use numerical indices in ef rather than strings, and the input v
> ## to ef has no names, we have to add them when requested
> ef <- function(v,addnames) {
+   w <- c(v[6]-v[5],v[7000]+v[5],v[7000]-v[6000])
+   if(addnames) names(w) <-c('id6-id5','f2k+id5','f2k-f1k')
+   w
+ }
> is.estimable(ef,list(id=id,firm=firm))
[1] TRUE
> 
> ## Then make an error; in the last coordinate, sum two firms
> ef <- function(v,addnames) {
+   w <- c(v[6]-v[5],v[7000]+v[5],v[7000]+v[6000])
+   if(addnames) names(w) <-c('id6-id5','f2k+id5','f2k-f1k')
+   w
+ }
> is.estimable(ef, list(id=id,firm=firm), keepdiff=TRUE)
Warning in is.estimable(ef, list(id = id, firm = firm), keepdiff = TRUE) :
  non-estimable function, largest error 0.002 in coordinate 3 ("f2k-f1k")
[1] FALSE
attr(,"diff")
      id6-id5       f2k+id5       f2k-f1k 
-1.096185e-10  1.217011e-10  1.771467e-03 
> options(oldopts)
> 
> 
> 
> 
> cleanEx()
> nameEx("kaczmarz")
> ### * kaczmarz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kaczmarz
> ### Title: Solve a linear system defined by factors
> ### Aliases: kaczmarz
> 
> ### ** Examples
> 
> 
> ## create factors
>   f1 <- factor(sample(24000,100000,replace=TRUE))
>   f2 <- factor(sample(20000,length(f1),replace=TRUE))
>   f3 <- factor(sample(10000,length(f1),replace=TRUE))
>   f4 <- factor(sample(8000,length(f1),replace=TRUE))
> ## the matrix of dummies
>   D <- makeDmatrix(list(f1,f2,f3,f4))
>   dim(D)
[1] 100000  61490
> ## an x
>   truex <- runif(ncol(D))
> ## and the right hand side
>   R <- as.vector(D %*% truex)
> ## solve it
>   sol <- kaczmarz(list(f1,f2,f3,f4),R)
> ## verify that the solution solves the system Dx = R
>   sqrt(sum((D %*% sol - R)^2))
[1] 2.804356e-07
> ## but the solution is not equal to the true x, because the system is
> ## underdetermined
>   sqrt(sum((sol - truex)^2))
[1] 52.72395
> ## moreover, the solution from kaczmarz has smaller norm
>   sqrt(sum(sol^2)) < sqrt(sum(truex^2))
[1] TRUE
> 
> 
> 
> 
> cleanEx()
> nameEx("lfe-package")
> ### * lfe-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lfe-package
> ### Title: Overview. Linear Group Fixed Effects
> ### Aliases: lfe lfe-package
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>   oldopts <- options(lfe.threads=1)
>   x <- rnorm(1000)
>   x2 <- rnorm(length(x))
>   id <- factor(sample(10,length(x),replace=TRUE))
>   firm <- factor(sample(3,length(x),replace=TRUE,prob=c(2,1.5,1)))
>   year <- factor(sample(10,length(x),replace=TRUE,prob=c(2,1.5,rep(1,8))))
>   id.eff <- rnorm(nlevels(id))
>   firm.eff <- rnorm(nlevels(firm))
>   year.eff <- rnorm(nlevels(year))
>   y <- x + 0.25*x2 + id.eff[id] + firm.eff[firm] +
+          year.eff[year] + rnorm(length(x))
>   est <- felm(y ~ x+x2 | id + firm + year)
>   summary(est)

Call:
   felm(formula = y ~ x + x2 | id + firm + year) 

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7847 -0.6308 -0.0100  0.6685  2.6419 

Coefficients:
   Estimate Std. Error t value Pr(>|t|)    
x   0.99805    0.03080  32.407  < 2e-16 ***
x2  0.25640    0.03079   8.326 2.79e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9983 on 977 degrees of freedom
Multiple R-squared(full model): 0.795   Adjusted R-squared: 0.7904 
Multiple R-squared(proj model): 0.5346   Adjusted R-squared: 0.5241 
F-statistic(full model):172.2 on 22 and 977 DF, p-value: < 2.2e-16 
F-statistic(proj model): 561.1 on 2 and 977 DF, p-value: < 2.2e-16 
*** Standard errors may be too high due to more than 2 groups and exactDOF=FALSE


> 
>   getfe(est,se=TRUE)
            effect obs comp   fe idx         se
id.1    -0.1040100  99    1   id   1 0.11606112
id.2    -0.4122123  87    1   id   2 0.13612384
id.3    -1.2554127 110    1   id   3 0.13905317
id.4    -1.0318285 107    1   id   4 0.12376648
id.5    -0.4003577  95    1   id   5 0.13273354
id.6    -1.4981340 107    1   id   6 0.11165320
id.7    -0.2455824  83    1   id   7 0.13198117
id.8    -0.2301982  94    1   id   8 0.12842267
id.9    -1.0591430 102    1   id   9 0.14391133
id.10   -0.8622197 116    1   id  10 0.11375803
firm.1   0.0000000 415    1 firm   1 0.00000000
firm.2   2.0570303 362    1 firm   2 0.06966295
firm.3  -0.7119418 223    1 firm   3 0.08542294
year.1   0.0000000 162    2 year   1 0.00000000
year.2   2.3920443 120    2 year   2 0.10840401
year.3  -0.9493803  82    2 year   3 0.14561771
year.4   1.9013068  92    2 year   4 0.12829532
year.5  -0.9171550  88    2 year   5 0.12735290
year.6   0.8661089  94    2 year   6 0.11749860
year.7   0.6138608  82    2 year   7 0.12246494
year.8   0.2385705  79    2 year   8 0.14429578
year.9  -0.2289633  90    2 year   9 0.13494791
year.10  1.0767303 111    2 year  10 0.12728775
> # compare with an ordinary lm
>   summary(lm(y ~ x+x2+id+firm+year-1))

Call:
lm(formula = y ~ x + x2 + id + firm + year - 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7847 -0.6308 -0.0100  0.6685  2.6419 

Coefficients:
       Estimate Std. Error t value Pr(>|t|)    
x       0.99805    0.03080  32.407  < 2e-16 ***
x2      0.25640    0.03079   8.326 2.79e-16 ***
id1    -0.10401    0.12840  -0.810  0.41812    
id2    -0.41221    0.13607  -3.029  0.00251 ** 
id3    -1.25541    0.12435 -10.096  < 2e-16 ***
id4    -1.03183    0.12672  -8.143 1.17e-15 ***
id5    -0.40036    0.13020  -3.075  0.00216 ** 
id6    -1.49813    0.12743 -11.757  < 2e-16 ***
id7    -0.24558    0.13633  -1.801  0.07195 .  
id8    -0.23020    0.13105  -1.757  0.07931 .  
id9    -1.05914    0.13210  -8.018 3.06e-15 ***
id10   -0.86222    0.12265  -7.030 3.88e-12 ***
firm2   2.05703    0.07231  28.448  < 2e-16 ***
firm3  -0.71194    0.08338  -8.538  < 2e-16 ***
year2   2.39204    0.12067  19.822  < 2e-16 ***
year3  -0.94938    0.13579  -6.991 5.04e-12 ***
year4   1.90131    0.13113  14.500  < 2e-16 ***
year5  -0.91715    0.13313  -6.889 1.00e-11 ***
year6   0.86611    0.13003   6.661 4.53e-11 ***
year7   0.61386    0.13629   4.504 7.47e-06 ***
year8   0.23857    0.13809   1.728  0.08438 .  
year9  -0.22896    0.13165  -1.739  0.08231 .  
year10  1.07673    0.12389   8.691  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9983 on 977 degrees of freedom
Multiple R-squared:  0.801,	Adjusted R-squared:  0.7964 
F-statistic:   171 on 23 and 977 DF,  p-value: < 2.2e-16

>   options(oldopts)
> 
> 
> 
> 
> cleanEx()
> nameEx("makeDmatrix")
> ### * makeDmatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeDmatrix
> ### Title: Make sparse matrix of dummies from factor list
> ### Aliases: makeDmatrix
> 
> ### ** Examples
> 
> 
>   fl <- lapply(1:3, function(i) factor(sample(3,10,replace=TRUE)))
>   fl
[[1]]
 [1] 1 2 2 3 1 3 3 2 2 1
Levels: 1 2 3

[[2]]
 [1] 1 1 3 2 3 2 3 3 2 3
Levels: 1 2 3

[[3]]
 [1] 3 1 2 1 1 2 1 2 3 2
Levels: 1 2 3

>   makeDmatrix(fl, weights=seq(0.1,1,0.1))
10 x 9 sparse Matrix of class "dgCMatrix"
      f1.1 f1.2 f1.3 f2.1 f2.2 f2.3 f3.1 f3.2 f3.3
 [1,]  0.1  .    .    0.1  .    .    .    .    0.1
 [2,]  .    0.2  .    0.2  .    .    0.2  .    .  
 [3,]  .    0.3  .    .    .    0.3  .    0.3  .  
 [4,]  .    .    0.4  .    0.4  .    0.4  .    .  
 [5,]  0.5  .    .    .    .    0.5  0.5  .    .  
 [6,]  .    .    0.6  .    0.6  .    .    0.6  .  
 [7,]  .    .    0.7  .    .    0.7  0.7  .    .  
 [8,]  .    0.8  .    .    .    0.8  .    0.8  .  
 [9,]  .    0.9  .    .    0.9  .    .    .    0.9
[10,]  1.0  .    .    .    .    1.0  .    1.0  .  
> 
> 
> 
> 
> cleanEx()
> nameEx("mctrace")
> ### * mctrace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mctrace
> ### Title: Compute trace of a large matrix by sample means
> ### Aliases: mctrace
> 
> ### ** Examples
> 
> 
>   A <- matrix(rnorm(25),5)
>   fun <- function(x) A %*% x
>   sum(diag(A))
[1] 0.6807816
>   sum(eigen(A,only.values=TRUE)$values)
[1] 0.6807816+0i
>   # mctrace is not really useful for small problems.
>   mctrace(fun,ncol(A),tol=0.05)
[1] 0.6560292
attr(,"sd")
[1] 0.03258954
attr(,"iterations")
[1] 10868
>   # try a larger problem (3000x3000):
>   f1 <- factor(sample(1500,3000,replace=TRUE))
>   f2 <- factor(sample(1500,3000,replace=TRUE))
>   fl <- list(f1,f2)
>   mctrace(fl,tol=-5)
[1] 512.4893
attr(,"sd")
[1] 4.026014
attr(,"iterations")
[1] 44
>   # exact:
>   length(f1) - nlevels(f1) - nlevels(f2) + nlevels(compfactor(fl))
[1] 507
> 
> 
> 
> 
> cleanEx()
> nameEx("nlexpect")
> ### * nlexpect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nlexpect
> ### Title: Compute expectation of a function of the coefficients.
> ### Aliases: nlexpect
> 
> ### ** Examples
> 
> 
> N <- 100
> x1 <- rnorm(N)
> # make some correlation
> x2 <- 0.1*rnorm(N) + 0.1*x1
> y <- 0.1*x1 + x2 + rnorm(N)
> summary(est <- felm(y ~ x1 + x2))

Call:
   felm(formula = y ~ x1 + x2) 

Residuals:
     Min       1Q   Median       3Q      Max 
-2.94359 -0.43645  0.00202  0.63692  2.63941 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.02535    0.10519   0.241    0.810
x1           0.17458    0.15997   1.091    0.278
x2           0.46533    1.09479   0.425    0.672

Residual standard error: 1.043 on 97 degrees of freedom
Multiple R-squared(full model): 0.03737   Adjusted R-squared: 0.01752 
Multiple R-squared(proj model): 0.03737   Adjusted R-squared: 0.01752 
F-statistic(full model):1.883 on 2 and 97 DF, p-value: 0.1577 
F-statistic(proj model): 1.883 on 2 and 97 DF, p-value: 0.1577 


> pt1 <- coef(est)['x1']
> pt2 <- coef(est)['x2']
> # expected values of coefficients, should match the summary
> # and variance, i.e. square of standard errors in the summary
> nlexpect(est, quote(c(x1=x1,x2=x2,var=c((x1-pt1)^2,(x2-pt2)^2))))
        x1         x2     var.x1     var.x2 
0.17451096 0.46528974 0.02610547 1.22465745 
> ## No test: 
> # the covariance matrix:
> nlexpect(est, tcrossprod(as.matrix(c(x1-pt1,x2-pt2))))
            x1         x2
x1  0.02612555 -0.1222348
x2 -0.12223476  1.2249513
> ## End(No test)
> #Wald test of single variable
> waldtest(est, ~x1)['p.F']
      p.F 
0.2778482 
> # the same with nlexpect, i.e. probability for observing abs(x1)>abs(pt1) conditional
> # on E(x1) = 0.
> nlexpect(est, (x1-pt1)^2 > pt1^2)
       x1 
0.2779127 
> # which of course is the same as
> 2*nlexpect(est, x1 < 0)
[1] 0.2778272
> 
> ## No test: 
> # then a joint test. Here we find the probability that
> # we are further from origo than the point estimates, measured
> # in a variance normalized coordinate system.
> waldtest(est, ~ x1 | x2)['p.F']
     p.F 
0.157677 
> #inverse cholesky
> ich <- solve(t(chol(vcov(est)[c('x1','x2'), c('x1','x2')])))
> # convert to normalized coordinates
> nz <- sum((ich %*% c(pt1,pt2))^2)
> # find probability that we're further away
> nlexpect(est, sum((ich %*% c(x1-pt1,x2-pt2))^2) > nz)
[1] 0.1577703
> # or use the .z argument provided automatically
> nlexpect(est, sum(.z^2) > nz, coefs=c('x1','x2'))
[1] 0.1577703
> 
> # Non-linear test:
> f <- function(x) c(poly=x[['x1']]*(6*x[['x1']]-x[['x2']]^2))
> waldtest(est, f)['p.F']
      p.F 
0.7432511 
> # In general, for a function f, the non-linear Wald test is something like
> # the following:
> # expected value of function
> Ef <- nlexpect(est, f, coefs=c('x1','x2'))
> # point value of function
> Pf <- f(c(pt1,pt2))
> # similar to a Wald test:
> nlexpect(est, function(x) (f(x)-Ef)^2 > Pf^2, c('x1','x2'))
     poly 
0.6828497 
> # one-sided
> nlexpect(est, function(x) f(x)-Ef > abs(Pf), c('x1','x2'))
     poly 
0.2683363 
> # other sided
> nlexpect(est, function(x) f(x)-Ef < -abs(Pf), c('x1','x2'))
     poly 
0.4144826 
> ## End(No test)
> 
> 
> 
> 
> cleanEx()
> nameEx("varvars")
> ### * varvars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varvars
> ### Title: Compute the variance of the fixed effect variance estimate
> ### Aliases: varvars
> 
> ### ** Examples
> 
> 
> x <- rnorm(500)
> x2 <- rnorm(length(x))
> 
> ## create individual and firm
> id <- factor(sample(40,length(x),replace=TRUE))
> firm <- factor(sample(30,length(x),replace=TRUE,prob=c(2,rep(1,29))))
> foo <- factor(sample(20,length(x),replace=TRUE))
> ## effects
> id.eff <- rnorm(nlevels(id))
> firm.eff <- rnorm(nlevels(firm))
> foo.eff <- rnorm(nlevels(foo))
> ## left hand side
> id.m <- id.eff[id]
> firm.m <- 2*firm.eff[firm]
> foo.m <- 3*foo.eff[foo]
> y <- x + 0.25*x2 + id.m + firm.m + foo.m + rnorm(length(x))
> 
> # make a data frame
> fr <- data.frame(y,x,x2,id,firm,foo)
> ## estimate and print result
> est <- felm(y ~ x+x2|id+firm+foo, data=fr, keepX=TRUE)
> alpha <- getfe(est)
> # estimate the covariance matrix of the fixed effects
> fevcov(est, alpha)
              id        firm         foo
id    0.95289256 -0.04488015  0.03963543
firm -0.04488015  5.53609420 -0.01160888
foo   0.03963543 -0.01160888  9.15619224
attr(,"bias")
               id         firm          foo
id    0.088877778 -0.005940383 -0.004116515
firm -0.005940383  0.082297201 -0.001586703
foo  -0.004116515 -0.001586703  0.051505257
> # estimate variances of the diagonal
> varvars(est, alpha)
[1] 0.01018015 0.05503299 0.09774750
> 
> 
> 
> 
> cleanEx()
> nameEx("waldtest")
> ### * waldtest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: waldtest
> ### Title: Compute Wald test for joint restrictions on coefficients
> ### Aliases: waldtest
> 
> ### ** Examples
> 
> 
> x <- rnorm(10000)
> x2 <- rnorm(length(x))
> y <- x - 0.2*x2 + rnorm(length(x))
> #Also works for lm
> summary(est <- lm(y ~ x + x2  ))

Call:
lm(formula = y ~ x + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.6477 -0.6843  0.0158  0.6744  3.6442 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.007554   0.010078    0.75    0.454    
x            0.999265   0.009956  100.37   <2e-16 ***
x2          -0.199066   0.010172  -19.57   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.008 on 9997 degrees of freedom
Multiple R-squared:  0.5108,	Adjusted R-squared:  0.5107 
F-statistic:  5219 on 2 and 9997 DF,  p-value: < 2.2e-16

> # We do not reject the true values
> waldtest(est, ~ x-1|x2+0.2|`(Intercept)`)
           p         chi2          df1          p.F            F          df2 
   0.9019639    0.5757362    3.0000000    0.9019611    0.1919121 9997.0000000 
attr(,"formula")
~x - 1 | x2 + 0.2 | `(Intercept)`
<environment: 0x9ef9208>
> # The Delta-method coincides when the function is linear:
> waldtest(est, function(x) x - c(0, 1, -0.2))
           p         chi2          df1          p.F            F          df2 
   0.9019639    0.5757362    3.0000000    0.9019611    0.1919121 9997.0000000 
attr(,"formula")
function (x) 
x - c(0, 1, -0.2)
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  23.63 4.255 21.933 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
